{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%pip install tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.models import Sequential\n","\n","# Define the model architecture\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_dim, input_length=max_seq_length))\n","model.add(LSTM(units=128))\n","model.add(Dense(vocab_size, activation='softmax'))\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=10, batch_size=32)\n","\n","# Generate text using the trained model\n","seed_text = \"Once upon a time\"\n","for _ in range(10):\n","    encoded_text = tokenizer.texts_to_sequences([seed_text])[0]\n","    encoded_text = tf.keras.preprocessing.sequence.pad_sequences([encoded_text], maxlen=max_seq_length-1, padding='pre')\n","    predicted_index = model.predict_classes(encoded_text)\n","    predicted_word = tokenizer.index_word[predicted_index[0]]\n","    seed_text += \" \" + predicted_word\n","\n","print(seed_text)"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":2}
